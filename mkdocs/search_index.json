{
    "docs": [
        {
            "location": "/",
            "text": "ARIS Exercises\n\n\nLogin to ARIS\n\n\nGet your login credentials\n\n\n\n\n\n\nObtain your training username and account.\n\n\n\n\n\n\nDownload private ssh key : \nlink\n\n\n\n\n\n\nLogin with SSH\n\n\nARIS supercomputer uses login nodes for interactive access and the submission of batch jobs.\n\n\nConnect to ARIS login nodes via SSH :\n\n\nLinux/Mac\n\n\nOpen a terminal and run\n\n\n$ ssh -i /path/to/ssh/key -X <USERNAME>@login.aris.grnet.gr\n\n# -X enables X11 forwarding \n\n#in case of problems confirm your ssh key permissions\n$ chmod 600 /path/to/ssh/key\n\n\n\n\nwhere \n<USERNAME>\n is your ARIS training account.\n\n\nWindows\n\n\nMicrosoft Windows do not have a built-in SSH client. We recommend downloading the following software:\n\n\n\n\nSSH client - \nPUTTY\n\n\nconfiguration instructions for ARIS\n\n\n\n\n\n\nSSH client - \nMobaXterm\n\n\nSSH client - \nBitvise\n\n\nconfiguration instructions for ARIS\n\n\n\n\n\n\nFile client - \nWinSCP\n\n\n\n\n\n\n\nFor more information on how to login please read our documentation site \nhttp://doc.aris.grnet.gr/login/#login-to-aris\n\n\n\n\n\nUser Environment\n\n\n\n\n\n\nCheck the available software.\n\n\n$ module avail\n\n\n\n\n\n\n\nCheck the loaded modules.\n\n\n$ module list\n\n\n\n\n\n\n\nWhich is the default GNU compiler ?\n\n\n\n\n\n\nLoad the \ngnuplot\n tool. We will need this in order to make plots for Ex.2 \n\n\n$ module load gnuplot\n\n\n\n\n\n\n\nInspect the environment changes when loading the \ngnuplot\n module\n\n\n$ module show gnuplot\n\n\n\n\n\n\n\nCheck the available partitions.\n\n\n$ sinfo -s\n\n\n\n\n\n\n\nCheck your account information.\n\n\n$ mybudget\n\n$ myreport\n\n\n\n\n\n\n\nWhat are the limits for your account ?       \n\n\n\n\n\n\nInspect ARIS file systems\n\n\n$ mmlsquota --block-size 1G\n\n\n\n\n\n\n\nSwitch to your working directory in order to run the following exercises.        \n\n\n$ cd $WORKDIR\n\n$ pwd\n# /work/training/<USERNAME>\n\n\n\n\n\n\n\nCheck your disk usage\n\n\n# HOME file system\n$ du -chs $HOME\n\n# WORK file system\n$ du -chs $WORKDIR\n\n\n\n\n\n\n\nRunning jobs\n\n\nIn order to run jobs on ARIS you need an account to charge compute hours.\nReplace the \n<TRAINING_PROJECT>\n with your given training project account in the following examples.\n\n\nPrepare a SLURM script file\n\n\n\n\n\n\nCreate a sample SLURM script named \nmy_script.slurm\n\n\n#!/bin/bash\n#SBATCH --job-name=my_script    # Job name\n#SBATCH --ntasks=1              # Number of tasks\n#SBATCH --time=00:01:00         # Run time (hh:mm:ss) - 1 minute \n#SBATCH --account=<TRAINING_PROJECT>\n\nmodule load gnu intel intelmpi        #load any needed modules\n\necho \"Start at `date`\"\n\n# Job steps\nsrun hostname\nsrun sleep 30\n\necho \"End at `date`\"\n\n\n\n\n\n\n\nSubmit your script to execute on ARIS.\n\n\n$ sbatch my_script.slurm\n# Submitted batch job <JOBID>\n\n\n\n\n\n\n\nCheck your running jobs.\n\n\n$ squeue -u <USERNAME>\n\n\n\n\n\n\n\nInspect the output files.\n\n\n$ cat my_script-<JOBID>.out\n\n\n\n\n\n\n\nIf you want to cancel your running job.\n\n\n$ scancel <JOBID>\n\n# or cancel all your running jobs \n$ scancel -u <USERNAME>\n\n\n\n\n\n\n\nAccounting information for completed jobs (current day)\n\n\n$ sacct\n\n\n\n\n\n\n\nTry to execute the sample script on multiple nodes\n\n\n\n\n\n\nCreate the same simple SLURM script using the online ARIS script-template: \nhttp://doc.aris.grnet.gr/scripttemplate/\n\n\n\n\n\n\nFor more details about running parallel job on ARIS please read: \nhttp://doc.aris.grnet.gr/run/\n\n\n\n\n\n\nEx.1 Application Execution\n\n\nExamine a simple program that computes \u03c0 by Monte-Carlo integration of a quarter of a unit circle.\nGiven is the source code in \nC\n language, in four different runtime models.\n\n\n\n\nSerial program: \nserialpi.c\n\n\nMulti-threaded program (with OpenMP): \nomppi.c\n\n\nMulti-process program (with MPI): \nmpipi.c\n\n\nHybrid program (with MPI+OpenMP): mpiomppi.c\n\n\n\n\nConsult the documentation how to submit jobs for your programming model. \nhttp://doc.aris.grnet.gr/run/#runtime-models\n\nFollowing we will create a batch script for each runtime model.  \n\n\n\n\n\n\n\n\n\nGet the source using \ngit\n.\n\n\n$ git clone https://github.com/hpc-grnet-gr/aris_exercises.git \n$ cd aris_exercises\n\n\n\n\n\n\n\nBuild the code using \nmake\n.\n\n\n $ make\n\n # Check that you got all the executables\n # serialpi, omppi, mpipi, mpiomppi\n\n\n\n\n\n\n\nSerial job execution\n\n\nRun the serial program using 1 processor.\n\n\n\n\n\n\nInspect the file \nserial.slurm\n \n\n\n#!/bin/bash\n\n#SBATCH --job-name=pi# Job name\n#SBATCH --ntasks=1 # Total number of tasks\n#SBATCH --nodes=1 # Total number of nodes requested\n#SBATCH --ntasks-per-node=1 # Tasks per node\n#SBATCH --cpus-per-task=1 # Threads per task\n#SBATCH -t 00:10:00 # Run time (hh:mm:ss) - (max 48h)\n\n# Load any necessary modules\nmodule load intel\n\n# Launch the executable\n./serialpi\n\n\n\n\n\n\n\nSubmit your serial job.\n\n\n$ sbatch serial.slurm\n\n\n\n\n\n\n\nCheck the output files.\n\n\n\n\n\n\nOpenMP job execution\n\n\nRun the OpenMP program using 20 threads.\n\n\n\n\n\n\nInspect the file \nopenmp.slurm\n.\n\n\n#!/bin/bash\n\n#SBATCH --job-name=pi# Job name\n#SBATCH --ntasks=1 # Total number of tasks\n#SBATCH --nodes=1 # Total number of nodes requested\n#SBATCH --ntasks-per-node=1 # Tasks per node\n#SBATCH --cpus-per-task=20 # Threads per task\n#SBATCH -t 00:10:00 # Run time (hh:mm:ss) - (max 48h)\n\n# Load any necessary modules\nmodule load intel\n\nexport OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK\n\n# Launch the executable\nsrun omppi\n\n\n\n\n\n\n\nSubmit your openmp job.\n\n\n$ sbatch openmp.slurm\n\n\n\n\n\n\n\nMPI job execution\n\n\nRun the MPI program using 2 nodes.\n\n\n\n\n\n\nInspect the file \nmpi.slurm\n.\n\n\n  #!/bin/bash\n\n  #SBATCH --job-name=mpipi # Job name\n  #SBATCH --ntasks=2 # Total number of tasks\n  #SBATCH --nodes=1 # Total number of nodes requested\n  #SBATCH --ntasks-per-node=1 # Tasks per node\n  #SBATCH --cpus-per-task=1 # Threads per task(=1) for pure MPI\n  #SBATCH -t 00:10:00 # Run time (hh:mm:ss) - (max 48h)\n\n  # Load any necessary modules\n  module load intel\n  module load intelmpi\n\n  export I_MPI_FABRICS=shm:dapl\n\n  # Launch the executable\n  srun mpipi\n\n\n\n\n\n\n\nSubmit your \nmpi\n job.\n\n\n$ sbatch mpi.slurm\n\n\n\n\n\n\n\nHybrid job execution\n\n\nRun the hybrid program using 2 nodes and 20 threads per node.\n\n\n\n\n\n\nCorrect the following script file \nhybrid.slurm\n.\n\n\n#!/bin/bash\n\n#SBATCH --job-name=mpiomppi # Job name\n#SBATCH --ntasks=40 # Total number of tasks\n#SBATCH --nodes=2 # Total number of nodes requested\n#SBATCH --ntasks-per-node=1 # Tasks per node\n#SBATCH --cpus-per-task=20 # Threads per task(=1) for pure MPI\n#SBATCH -t 00:10:00 # Run time (hh:mm:ss) - (max 48h)\n\n# Load any necessary modules\nmodule load intel\nmodule load intelmpi\n\nexport I_MPI_FABRICS=shm:dapl\nexport OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK\n\n# Launch the executable\nsrun mpiomppi\n\n\n\n\n\n\n\nSubmit your \nhybrid\n job.\n\n\n$ sbatch hybrid.slurm\n\n\n\n\n\n\n\nEx.2 See how well your application scales.\n\n\n\n\nScalability is defined as the ability to handle more work as the size of the computer or application grows.\n\n\n\n\n\n\n\n\n\nStrong scaling\n : the problem size remains fixed while the number of nodes increases. (work per node decrease)\n\n\nWeak scaling\n: the problem gets bigger and bigger as the number of processors increases. (work per node same)\n\n\n\n\nGiven the code in the previous exercise above we shall test how well this application scales.\n\n\nStrong scaling test:\n\n\n\n\nKeeping the problem size constant, run MPI on only one processor. Compare to the serial calculation to get the overhead associated with MPI.\n\n\n\n\nKeeping the problem size run the MPI code for 1, 2, 4, 8 computing nodes.\n\n\nRun the the \nstrong_scale.sh\n script to get the timing results.\n\n\n$ bash strong_scale.sh\n\n\n\nCheck the status or your submitted jobs, output results will be saved in the \nmpipi-strong*.log\n files.\n\n\n$ squeue -i 5 -u <USERNAME>\n# press Ctrl + C to exit\n\n\n\n\n\n\n\nMake a plot of run time versus number of nodes.\n\n\nYou can use \ngnuplot\n to plot your results.\n\n\n# Gather result timings in one file\n$ cat mpipi-strong*.log | grep -v '^#' | sort -n -k 1 > mpipi-strong.dat\n\n# plot \n$ gnuplot strong_plot.gnuplot\n\n\n\nYou can view the output plot image \nmpipi-strong.png\n\n\n# If you have enabled X11 forwarding in SSH (-X) you can view the output\n$ display mpipi-strong.png\n\n# or from your laptop download the image file to view it localy\n# use the path to your ssh key\n$ scp -i /path/to/ssh/key <USERNAME>@login.aris.grnet.gr:/work/training/<USERNAME>/aris_exercises/mpipi-strong.png .\n\n\n\n\n\n\n\nStrong scalability would yield a straight line graph. Comment.\n\n\n\n\n\n\nWeak scaling test:\n\n\n\n\n\n\n\n\n\nRun the MPI code for 1, 2, 4, 8 computing nodes, with proportionally larger values in each case (2000, 4000, ...)\n\n\nRun the \nweak_scale.sh\n script to get the timing results.\n\n\n$ bash weak_scale.sh\n\n\n\nCheck the status or your submitted jobs, output results will be saved in the mpipi-weak*.log files.\n\n\n\n\n\n\nMake a plot of the run time versus number of computing nodes.\n\n\n$ cat mpipi-weak*.log | grep -v '^#' | sort -n -k 1 > mpipi-weak.dat\n$ gnuplot weak_plot.gnuplot\n\n\n\nOutput plot \nmpipi-weak.png\n\n\n# If you have enabled X11 forwarding in SSH (-X) you can view the output\n$ display mpipi-weak.png\n\n# or from your laptop download the image file to view it localy\n$ scp -i /path/to/ssh/key  <USERNAME>@login.aris.grnet.gr:/work/training/<USERNAME>/aris_exercises/mpipi-weak.png .\n\n\n\n\n\n\n\nWeak scaling would imply that the run time remains constant as the problem size and the nummber of compute nodes increase in proportion. Comment.\n\n\n\n\nIs this problem more appropriate for weak or strong scaling ?",
            "title": "Home"
        },
        {
            "location": "/#aris-exercises",
            "text": "",
            "title": "ARIS Exercises"
        },
        {
            "location": "/#login-to-aris",
            "text": "Get your login credentials    Obtain your training username and account.    Download private ssh key :  link    Login with SSH  ARIS supercomputer uses login nodes for interactive access and the submission of batch jobs.  Connect to ARIS login nodes via SSH :  Linux/Mac  Open a terminal and run  $ ssh -i /path/to/ssh/key -X <USERNAME>@login.aris.grnet.gr\n\n# -X enables X11 forwarding \n\n#in case of problems confirm your ssh key permissions\n$ chmod 600 /path/to/ssh/key  where  <USERNAME>  is your ARIS training account.  Windows  Microsoft Windows do not have a built-in SSH client. We recommend downloading the following software:   SSH client -  PUTTY  configuration instructions for ARIS    SSH client -  MobaXterm  SSH client -  Bitvise  configuration instructions for ARIS    File client -  WinSCP    For more information on how to login please read our documentation site  http://doc.aris.grnet.gr/login/#login-to-aris",
            "title": "Login to ARIS"
        },
        {
            "location": "/#user-environment",
            "text": "Check the available software.  $ module avail    Check the loaded modules.  $ module list    Which is the default GNU compiler ?    Load the  gnuplot  tool. We will need this in order to make plots for Ex.2   $ module load gnuplot    Inspect the environment changes when loading the  gnuplot  module  $ module show gnuplot    Check the available partitions.  $ sinfo -s    Check your account information.  $ mybudget\n\n$ myreport    What are the limits for your account ?           Inspect ARIS file systems  $ mmlsquota --block-size 1G    Switch to your working directory in order to run the following exercises.          $ cd $WORKDIR\n\n$ pwd\n# /work/training/<USERNAME>    Check your disk usage  # HOME file system\n$ du -chs $HOME\n\n# WORK file system\n$ du -chs $WORKDIR",
            "title": "User Environment"
        },
        {
            "location": "/#running-jobs",
            "text": "In order to run jobs on ARIS you need an account to charge compute hours.\nReplace the  <TRAINING_PROJECT>  with your given training project account in the following examples.  Prepare a SLURM script file    Create a sample SLURM script named  my_script.slurm  #!/bin/bash\n#SBATCH --job-name=my_script    # Job name\n#SBATCH --ntasks=1              # Number of tasks\n#SBATCH --time=00:01:00         # Run time (hh:mm:ss) - 1 minute \n#SBATCH --account=<TRAINING_PROJECT>\n\nmodule load gnu intel intelmpi        #load any needed modules\n\necho \"Start at `date`\"\n\n# Job steps\nsrun hostname\nsrun sleep 30\n\necho \"End at `date`\"    Submit your script to execute on ARIS.  $ sbatch my_script.slurm\n# Submitted batch job <JOBID>    Check your running jobs.  $ squeue -u <USERNAME>    Inspect the output files.  $ cat my_script-<JOBID>.out    If you want to cancel your running job.  $ scancel <JOBID>\n\n# or cancel all your running jobs \n$ scancel -u <USERNAME>    Accounting information for completed jobs (current day)  $ sacct    Try to execute the sample script on multiple nodes    Create the same simple SLURM script using the online ARIS script-template:  http://doc.aris.grnet.gr/scripttemplate/    For more details about running parallel job on ARIS please read:  http://doc.aris.grnet.gr/run/",
            "title": "Running jobs"
        },
        {
            "location": "/#ex1-application-execution",
            "text": "Examine a simple program that computes \u03c0 by Monte-Carlo integration of a quarter of a unit circle.\nGiven is the source code in  C  language, in four different runtime models.   Serial program:  serialpi.c  Multi-threaded program (with OpenMP):  omppi.c  Multi-process program (with MPI):  mpipi.c  Hybrid program (with MPI+OpenMP): mpiomppi.c   Consult the documentation how to submit jobs for your programming model.  http://doc.aris.grnet.gr/run/#runtime-models \nFollowing we will create a batch script for each runtime model.       Get the source using  git .  $ git clone https://github.com/hpc-grnet-gr/aris_exercises.git \n$ cd aris_exercises    Build the code using  make .   $ make\n\n # Check that you got all the executables\n # serialpi, omppi, mpipi, mpiomppi    Serial job execution  Run the serial program using 1 processor.    Inspect the file  serial.slurm    #!/bin/bash\n\n#SBATCH --job-name=pi# Job name\n#SBATCH --ntasks=1 # Total number of tasks\n#SBATCH --nodes=1 # Total number of nodes requested\n#SBATCH --ntasks-per-node=1 # Tasks per node\n#SBATCH --cpus-per-task=1 # Threads per task\n#SBATCH -t 00:10:00 # Run time (hh:mm:ss) - (max 48h)\n\n# Load any necessary modules\nmodule load intel\n\n# Launch the executable\n./serialpi    Submit your serial job.  $ sbatch serial.slurm    Check the output files.    OpenMP job execution  Run the OpenMP program using 20 threads.    Inspect the file  openmp.slurm .  #!/bin/bash\n\n#SBATCH --job-name=pi# Job name\n#SBATCH --ntasks=1 # Total number of tasks\n#SBATCH --nodes=1 # Total number of nodes requested\n#SBATCH --ntasks-per-node=1 # Tasks per node\n#SBATCH --cpus-per-task=20 # Threads per task\n#SBATCH -t 00:10:00 # Run time (hh:mm:ss) - (max 48h)\n\n# Load any necessary modules\nmodule load intel\n\nexport OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK\n\n# Launch the executable\nsrun omppi    Submit your openmp job.  $ sbatch openmp.slurm    MPI job execution  Run the MPI program using 2 nodes.    Inspect the file  mpi.slurm .    #!/bin/bash\n\n  #SBATCH --job-name=mpipi # Job name\n  #SBATCH --ntasks=2 # Total number of tasks\n  #SBATCH --nodes=1 # Total number of nodes requested\n  #SBATCH --ntasks-per-node=1 # Tasks per node\n  #SBATCH --cpus-per-task=1 # Threads per task(=1) for pure MPI\n  #SBATCH -t 00:10:00 # Run time (hh:mm:ss) - (max 48h)\n\n  # Load any necessary modules\n  module load intel\n  module load intelmpi\n\n  export I_MPI_FABRICS=shm:dapl\n\n  # Launch the executable\n  srun mpipi    Submit your  mpi  job.  $ sbatch mpi.slurm    Hybrid job execution  Run the hybrid program using 2 nodes and 20 threads per node.    Correct the following script file  hybrid.slurm .  #!/bin/bash\n\n#SBATCH --job-name=mpiomppi # Job name\n#SBATCH --ntasks=40 # Total number of tasks\n#SBATCH --nodes=2 # Total number of nodes requested\n#SBATCH --ntasks-per-node=1 # Tasks per node\n#SBATCH --cpus-per-task=20 # Threads per task(=1) for pure MPI\n#SBATCH -t 00:10:00 # Run time (hh:mm:ss) - (max 48h)\n\n# Load any necessary modules\nmodule load intel\nmodule load intelmpi\n\nexport I_MPI_FABRICS=shm:dapl\nexport OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK\n\n# Launch the executable\nsrun mpiomppi    Submit your  hybrid  job.  $ sbatch hybrid.slurm",
            "title": "Ex.1 Application Execution"
        },
        {
            "location": "/#ex2-see-how-well-your-application-scales",
            "text": "Scalability is defined as the ability to handle more work as the size of the computer or application grows.     Strong scaling  : the problem size remains fixed while the number of nodes increases. (work per node decrease)  Weak scaling : the problem gets bigger and bigger as the number of processors increases. (work per node same)   Given the code in the previous exercise above we shall test how well this application scales.  Strong scaling test:   Keeping the problem size constant, run MPI on only one processor. Compare to the serial calculation to get the overhead associated with MPI.   Keeping the problem size run the MPI code for 1, 2, 4, 8 computing nodes.  Run the the  strong_scale.sh  script to get the timing results.  $ bash strong_scale.sh  Check the status or your submitted jobs, output results will be saved in the  mpipi-strong*.log  files.  $ squeue -i 5 -u <USERNAME>\n# press Ctrl + C to exit    Make a plot of run time versus number of nodes.  You can use  gnuplot  to plot your results.  # Gather result timings in one file\n$ cat mpipi-strong*.log | grep -v '^#' | sort -n -k 1 > mpipi-strong.dat\n\n# plot \n$ gnuplot strong_plot.gnuplot  You can view the output plot image  mpipi-strong.png  # If you have enabled X11 forwarding in SSH (-X) you can view the output\n$ display mpipi-strong.png\n\n# or from your laptop download the image file to view it localy\n# use the path to your ssh key\n$ scp -i /path/to/ssh/key <USERNAME>@login.aris.grnet.gr:/work/training/<USERNAME>/aris_exercises/mpipi-strong.png .    Strong scalability would yield a straight line graph. Comment.    Weak scaling test:     Run the MPI code for 1, 2, 4, 8 computing nodes, with proportionally larger values in each case (2000, 4000, ...)  Run the  weak_scale.sh  script to get the timing results.  $ bash weak_scale.sh  Check the status or your submitted jobs, output results will be saved in the mpipi-weak*.log files.    Make a plot of the run time versus number of computing nodes.  $ cat mpipi-weak*.log | grep -v '^#' | sort -n -k 1 > mpipi-weak.dat\n$ gnuplot weak_plot.gnuplot  Output plot  mpipi-weak.png  # If you have enabled X11 forwarding in SSH (-X) you can view the output\n$ display mpipi-weak.png\n\n# or from your laptop download the image file to view it localy\n$ scp -i /path/to/ssh/key  <USERNAME>@login.aris.grnet.gr:/work/training/<USERNAME>/aris_exercises/mpipi-weak.png .    Weak scaling would imply that the run time remains constant as the problem size and the nummber of compute nodes increase in proportion. Comment.   Is this problem more appropriate for weak or strong scaling ?",
            "title": "Ex.2 See how well your application scales."
        }
    ]
}